# requirements.txt
torch>=2.0.0
torchvision>=0.15.0
fastapi>=0.104.0
uvicorn>=0.24.0
opencv-python>=4.8.0
numpy>=1.24.0
scipy>=1.11.0
pillow>=10.0.0
open3d>=0.17.0
trimesh>=3.23.0
scikit-image>=0.21.0
pydantic>=2.4.0
redis>=5.0.0
celery>=5.3.0
matplotlib>=3.7.0
pyyaml>=6.0.1
pytest>=7.4.0
black>=23.9.0
flake8>=6.1.0

# src/core/sketch_processor.py
import cv2
import numpy as np
from PIL import Image
import torch
import torch.nn.functional as F
from typing import Tuple, Optional, Dict, Any
import logging
from pathlib import Path

from ..utils.image_processing import normalize_sketch, clean_sketch
from ..utils.validation import validate_sketch_input
from ..models.sketch_classifier import SketchClassifier
from ..models.depth_estimator import DepthEstimator

logger = logging.getLogger(__name__)

class SketchProcessor:
    """
    Processeur principal pour convertir les sketches en modèles 3D.
    Pipeline: Sketch → Classification → Depth → 3D Mesh
    """
    
    def __init__(self, model_config: Dict[str, Any]):
        self.config = model_config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Chargement des modèles
        self.classifier = self._load_classifier()
        self.depth_estimator = self._load_depth_estimator()
        
        logger.info(f"SketchProcessor initialisé sur {self.device}")
    
    def _load_classifier(self) -> SketchClassifier:
        """Charge le modèle de classification des sketches"""
        model = SketchClassifier(
            num_classes=self.config['classifier']['num_classes'],
            backbone=self.config['classifier']['backbone']
        )
        
        weights_path = Path(self.config['classifier']['weights_path'])
        if weights_path.exists():
            checkpoint = torch.load(weights_path, map_location=self.device)
            model.load_state_dict(checkpoint['model_state_dict'])
            logger.info("Modèle de classification chargé avec succès")
        else:
            logger.warning("Poids de classification non trouvés, utilisation du modèle non-entraîné")
        
        model.to(self.device)
        model.eval()
        return model
    
    def _load_depth_estimator(self) -> DepthEstimator:
        """Charge le modèle d'estimation de profondeur"""
        model = DepthEstimator(
            input_channels=1,
            output_channels=1,
            features=self.config['depth_estimator']['features']
        )
        
        weights_path = Path(self.config['depth_estimator']['weights_path'])
        if weights_path.exists():
            checkpoint = torch.load(weights_path, map_location=self.device)
            model.load_state_dict(checkpoint['model_state_dict'])
            logger.info("Modèle d'estimation de profondeur chargé avec succès")
        else:
            logger.warning("Poids d'estimation de profondeur non trouvés")
        
        model.to(self.device)
        model.eval()
        return model
    
    def preprocess_sketch(self, sketch_image: np.ndarray) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """
        Préprocessing du sketch d'entrée
        
        Args:
            sketch_image: Image du sketch (H, W, C) ou (H, W)
            
        Returns:
            Tensor préprocessé et métadonnées
        """
        # Validation de l'entrée
        if not validate_sketch_input(sketch_image):
            raise ValueError("Sketch d'entrée invalide")
        
        # Conversion en niveaux de gris si nécessaire
        if len(sketch_image.shape) == 3:
            sketch_gray = cv2.cvtColor(sketch_image, cv2.COLOR_RGB2GRAY)
        else:
            sketch_gray = sketch_image.copy()
        
        # Nettoyage du sketch
        cleaned_sketch = clean_sketch(sketch_gray)
        
        # Normalisation et redimensionnement
        normalized_sketch = normalize_sketch(cleaned_sketch, target_size=512)
        
        # Conversion en tensor PyTorch
        sketch_tensor = torch.from_numpy(normalized_sketch).float()
        sketch_tensor = sketch_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)
        sketch_tensor = sketch_tensor.to(self.device)
        
        # Métadonnées
        metadata = {
            'original_shape': sketch_image.shape,
            'preprocessed_shape': normalized_sketch.shape,
            'has_content': np.sum(normalized_sketch > 0.1) > 100  # Seuil minimum de contenu
        }
        
        return sketch_tensor, metadata
    
    def classify_sketch(self, sketch_tensor: torch.Tensor) -> Dict[str, Any]:
        """
        Classification du type de sketch
        
        Args:
            sketch_tensor: Tensor du sketch préprocessé
            
        Returns:
            Dictionnaire avec la classification et la confiance
        """
        with torch.no_grad():
            logits = self.classifier(sketch_tensor)
            probabilities = F.softmax(logits, dim=1)
            confidence, predicted_class = torch.max(probabilities, 1)
            
            return {
                'class_id': predicted_class.item(),
                'confidence': confidence.item(),
                'class_name': self.config['classifier']['class_names'][predicted_class.item()],
                'probabilities': probabilities.cpu().numpy().flatten()
            }
    
    def estimate_depth(self, sketch_tensor: torch.Tensor, class_info: Dict[str, Any]) -> torch.Tensor:
        """
        Estimation de la carte de profondeur
        
        Args:
            sketch_tensor: Tensor du sketch
            class_info: Information de classification
            
        Returns:
            Carte de profondeur (1, 1, H, W)
        """
        with torch.no_grad():
            # Ajout d'information de classe comme conditioning
            depth_map = self.depth_estimator(sketch_tensor)
            
            # Post-traitement spécifique à la classe
            depth_map = self._postprocess_depth(depth_map, class_info)
            
            return depth_map
    
    def _postprocess_depth(self, depth_map: torch.Tensor, class_info: Dict[str, Any]) -> torch.Tensor:
        """
        Post-traitement de la carte de profondeur selon la classe d'objet
        """
        class_name = class_info['class_name']
        
        # Normalisation de base
        depth_map = torch.clamp(depth_map, 0, 1)
        
        # Ajustements spécifiques par classe
        if class_name in ['cube', 'box']:
            # Profondeur plus uniforme pour les objets cubiques
            depth_map = torch.where(depth_map > 0.1, 
                                  torch.clamp(depth_map * 1.2, 0, 1), 
                                  depth_map)
        elif class_name in ['sphere', 'circle']:
            # Profondeur lissée pour les objets sphériques
            depth_map = F.gaussian_blur(depth_map, kernel_size=5, sigma=1.0)
        
        return depth_map
    
    def process_sketch_to_3d(self, sketch_image: np.ndarray) -> Dict[str, Any]:
        """
        Pipeline complet de conversion sketch vers 3D
        
        Args:
            sketch_image: Image du sketch d'entrée
            
        Returns:
            Dictionnaire avec tous les résultats intermédiaires et finaux
        """
        logger.info("Début du traitement sketch-to-3D")
        
        try:
            # Étape 1: Préprocessing
            logger.info("Préprocessing du sketch...")
            sketch_tensor, preprocessing_meta = self.preprocess_sketch(sketch_image)
            
            if not preprocessing_meta['has_content']:
                raise ValueError("Le sketch ne contient pas suffisamment de contenu")
            
            # Étape 2: Classification
            logger.info("Classification du sketch...")
            classification_result = self.classify_sketch(sketch_tensor)
            
            if classification_result['confidence'] < self.config['min_confidence_threshold']:
                logger.warning(f"Confiance de classification faible: {classification_result['confidence']:.2f}")
            
            # Étape 3: Estimation de profondeur
            logger.info("Estimation de la profondeur...")
            depth_map = self.estimate_depth(sketch_tensor, classification_result)
            
            # Compilation des résultats
            results = {
                'status': 'success',
                'preprocessing': preprocessing_meta,
                'classification': classification_result,
                'depth_map': depth_map.cpu().numpy(),
                'sketch_tensor': sketch_tensor.cpu().numpy(),
                'processing_device': str(self.device)
            }
            
            logger.info("Traitement sketch-to-3D terminé avec succès")
            return results
            
        except Exception as e:
            logger.error(f"Erreur lors du traitement: {str(e)}")
            return {
                'status': 'error',
                'error_message': str(e),
                'error_type': type(e).__name__
            }

# src/models/sketch_classifier.py
import torch
import torch.nn as nn
import torchvision.models as models
from typing import Optional

class SketchClassifier(nn.Module):
    """
    Modèle de classification pour identifier le type d'objet dans un sketch
    """
    
    def __init__(self, num_classes: int = 345, backbone: str = 'resnet18', pretrained: bool = True):
        super(SketchClassifier, self).__init__()
        
        self.num_classes = num_classes
        self.backbone_name = backbone
        
        # Sélection du backbone
        if backbone == 'resnet18':
            self.backbone = models.resnet18(pretrained=pretrained)
            # Modification pour accepter les images en niveaux de gris
            self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
            feature_dim = self.backbone.fc.in_features
            self.backbone.fc = nn.Identity()  # Suppression de la couche finale
        elif backbone == 'mobilenet_v2':
            self.backbone = models.mobilenet_v2(pretrained=pretrained)
            self.backbone.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)
            feature_dim = self.backbone.classifier[1].in_features
            self.backbone.classifier = nn.Identity()
        else:
            raise ValueError(f"Backbone non supporté: {backbone}")
        
        # Tête de classification personnalisée
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(feature_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, num_classes)
        )
        
        # Initialisation des poids
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialisation des poids de la tête de classification"""
        for m in self.classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass
        
        Args:
            x: Tensor d'entrée (batch_size, 1, H, W)
            
        Returns:
            Logits de classification (batch_size, num_classes)
        """
        # Extraction des features
        features = self.backbone(x)
        
        # Classification
        if len(features.shape) > 2:
            features = torch.flatten(features, 1)
        
        logits = self.classifier(features)
        
        return logits

# src/models/depth_estimator.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class DepthEstimator(nn.Module):
    """
    Modèle U-Net pour l'estimation de profondeur à partir de sketches
    """
    
    def __init__(self, input_channels: int = 1, output_channels: int = 1, features: int = 64):
        super(DepthEstimator, self).__init__()
        
        self.input_channels = input_channels
        self.output_channels = output_channels
        
        # Encodeur (Contracting Path)
        self.enc1 = self._make_conv_block(input_channels, features)
        self.enc2 = self._make_conv_block(features, features * 2)
        self.enc3 = self._make_conv_block(features * 2, features * 4)
        self.enc4 = self._make_conv_block(features * 4, features * 8)
        
        # Bottleneck
        self.bottleneck = self._make_conv_block(features * 8, features * 16)
        
        # Décodeur (Expansive Path)
        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, 2, 2)
        self.dec4 = self._make_conv_block(features * 16, features * 8)
        
        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, 2, 2)
        self.dec3 = self._make_conv_block(features * 8, features * 4)
        
        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, 2, 2)
        self.dec2 = self._make_conv_block(features * 4, features * 2)
        
        self.upconv1 = nn.ConvTranspose2d(features * 2, features, 2, 2)
        self.dec1 = self._make_conv_block(features * 2, features)
        
        # Couche finale
        self.final_conv = nn.Conv2d(features, output_channels, 1)
        
        # Pool pour le downsampling
        self.pool = nn.MaxPool2d(2, 2)
        
    def _make_conv_block(self, in_channels: int, out_channels: int) -> nn.Module:
        """Crée un bloc de convolution standard"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass du U-Net
        
        Args:
            x: Tensor d'entrée (batch_size, input_channels, H, W)
            
        Returns:
            Carte de profondeur (batch_size, output_channels, H, W)
        """
        # Encodage avec skip connections
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool(enc1))
        enc3 = self.enc3(self.pool(enc2))
        enc4 = self.enc4(self.pool(enc3))
        
        # Bottleneck
        bottleneck = self.bottleneck(self.pool(enc4))
        
        # Décodage avec skip connections
        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat([dec4, enc4], dim=1)
        dec4 = self.dec4(dec4)
        
        dec3 = self.upconv3(dec4)
        dec3 = torch.cat([dec3, enc3], dim=1)
        dec3 = self.dec3(dec3)
        
        dec2 = self.upconv2(dec3)
        dec2 = torch.cat([dec2, enc2], dim=1)
        dec2 = self.dec2(dec2)
        
        dec1 = self.upconv1(dec2)
        dec1 = torch.cat([dec1, enc1], dim=1)
        dec1 = self.dec1(dec1)
        
        # Sortie finale avec activation sigmoid pour normaliser entre 0 et 1
        output = torch.sigmoid(self.final_conv(dec1))
        
        return output